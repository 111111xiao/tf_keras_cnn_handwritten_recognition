{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os \n",
    "import sys\n",
    "import time \n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for model in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(model.__name__, model.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./input/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seaborn是基于matplotlib的图形可视化python包\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style='white', context='notebook', palette='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#divide x, y from train_df\n",
    "\n",
    "y_train = train_df['label']\n",
    "\n",
    "#drop label column   axis = 1 == col; axis = 0 == row\n",
    "x_train = train_df.drop(labels = ['label'], axis = 1)\n",
    "\n",
    "#free space\n",
    "del train_df\n",
    "\n",
    "g = sns.countplot(y_train)\n",
    "\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the data\n",
    "x_train.isnull().any().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.isnull().any().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(x_train.shape)\n",
    "pprint.pprint(test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Normalization\n",
    "\n",
    "x_train = x_train / 255.0\n",
    "\n",
    "pprint.pprint(x_train.shape)\n",
    "pprint.pprint(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\n",
    "\n",
    "x_train = x_train.values.reshape(-1, 28, 28, 1)\n",
    "test_df = test_df.values.reshape(-1, 28, 28, 1)\n",
    "\n",
    "pprint.pprint(x_train.shape)\n",
    "pprint.pprint(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes = 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "random_seed = 2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the train and the validation set for the fitting\n",
    "# 10% for validation 90% for train\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_train, y_train, test_size = 0.1, random_state=random_seed)\n",
    "\n",
    "pprint.pprint(x_train.shape)\n",
    "pprint.pprint(x_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show example\n",
    "g = plt.imshow(x_train[2][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the CNN model \n",
    "# my CNN architechture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out\n",
    "#from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.layers.Conv2D(filters = 32, kernel_size = 5, padding = 'same',\n",
    "                 activation = 'relu', input_shape = (28, 28, 1)))\n",
    "model.add(keras.layers.Conv2D(filters = 32, kernel_size = 5, padding = 'same',\n",
    "                 activation = 'relu'))\n",
    "model.add(keras.layers.MaxPool2D(pool_size = 2))\n",
    "model.add(keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(keras.layers.Conv2D(filters = 64, kernel_size = 5, padding = 'same',\n",
    "                 activation = 'relu'))\n",
    "model.add(keras.layers.Conv2D(filters = 64, kernel_size = 5, padding = 'same',\n",
    "                 activation = 'relu'))\n",
    "model.add(keras.layers.MaxPool2D(pool_size = 2))\n",
    "model.add(keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(255, activation = 'relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(21, activation = 'softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer = optimizer ,\n",
    "              loss = \"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "batch_size = 86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With data augmentation to prevent overfitting (accuracy 0.99286)\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1)  # randomly shift images vertically (fraction of total height)\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = datagen.flow(x_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = './callbacks'\n",
    "if not os.path.exists(logdir):\n",
    "    os.mkdir(logdir)\n",
    "output_model_file = os.path.join(logdir,\n",
    "                                 \"mnist_model.h5\")\n",
    "\n",
    "callbacks = [\n",
    "keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                  patience=3, \n",
    "                                  verbose=1, \n",
    "                                  factor=0.5, \n",
    "                                  min_lr=0.00001)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history = model.fit_generator(train_generator,\n",
    "                              epochs = epochs,\n",
    "                              steps_per_epoch = x_train.shape[0] // batch_size,\n",
    "                              validation_data = (x_valid, y_valid),\n",
    "                              validation_steps = x_valid.shape[0] // batch_size,\n",
    "                              callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history, label, epochs, min_value, max_value):\n",
    "    data = {}\n",
    "    data[label] = history.history[label]\n",
    "    data['val_' + label] = history.history['val_' + label]\n",
    "    pd.DataFrame(data).plot(figsize = (8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.axis([0, epochs, min_value, max_value])\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(history, 'accuracy', epochs, 0.9, 1)\n",
    "plot_learning_curves(history, 'loss', epochs, 0, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize = False,\n",
    "                          title = 'Confusion Matrix',\n",
    "                          cmap = plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation = 45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(asix = 1)[:,np.newaxis]\n",
    "        \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], \n",
    "                 horizontalalignment = 'center',\n",
    "                 color = 'white' if cm[i, j] > thresh else 'black')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "# predict the values in validation dataset\n",
    "y_pred = model.predict(x_valid)\n",
    "# convert predictions classes to one hot vectors\n",
    "# argmax is position of max data\n",
    "y_pred_classes = np.argmax(y_pred, axis = 1)\n",
    "# convert validation observations to one hot vectors\n",
    "y_true = np.argmax(y_valid, axis = 1)\n",
    "# comput the confusion matrix\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(y_true, y_pred_classes)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_matrix, classes = range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some error results \n",
    "\n",
    "# Errors are difference between predicted labels and true labels\n",
    "errors = (y_pred_classes - y_true != 0)\n",
    "\n",
    "y_pred_classes_errors = y_pred_classes[errors]\n",
    "y_pred_errors = y_pred[errors]\n",
    "y_true_errors = y_true[errors]\n",
    "x_valid_errors = x_valid[errors]\n",
    "\n",
    "def display_errors(error_index, img_errors, pred_errors, obs_errors):\n",
    "    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n",
    "    n = 0\n",
    "    nrows = 2\n",
    "    ncols = 3\n",
    "    fig, ax = plt.subplots(nrows, ncols, sharex = True, sharey = True, figsize=(10, 7))\n",
    "    for row in range(nrows):\n",
    "        for col in range(ncols):\n",
    "            error = error_index[n]\n",
    "            ax[row][col].imshow((img_errors[error]).reshape((28, 28)))\n",
    "            ax[row][col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(\n",
    "                pred_errors[error], obs_errors[error]))\n",
    "            n += 1\n",
    "\n",
    "# Probabilities of wrong predicted numbers\n",
    "y_pred_errors_prob = np.max(y_pred_errors, axis = 1)\n",
    "\n",
    "# a = [0, 1, 1, 2]\n",
    "# b = np.diagonal(np.take([[0, 1, 8], [2, 3, 9], [4, 5, 10], [6, 7, 11]], a, axis = 1))\n",
    "# pprint.pprint(b)\n",
    "# Predicted probabilities of the true values in the error set\n",
    "true_prob_errors = np.diagonal(np.take(y_pred_errors, y_true_errors, axis = 1))\n",
    "\n",
    "# Different between the probabilities of predicetd label and true label\n",
    "delta_pred_true_errors = y_pred_errors_prob - true_prob_errors\n",
    "\n",
    "# Sorted list of the delta prob errors\n",
    "sorted_delta_errors = np.argsort(delta_pred_true_errors)\n",
    "\n",
    "# Top 6 errors\n",
    "most_important_errors = sorted_delta_errors[-6:]\n",
    "\n",
    "# Show the top 6 errors\n",
    "display_errors(most_important_errors, x_valid_errors, y_pred_classes_errors, y_true_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
